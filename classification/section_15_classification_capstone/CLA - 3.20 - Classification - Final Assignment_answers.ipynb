{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Final assignment\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You'll need to download the following files from the link: [statlog shuttle](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/)\n",
    "    * shuttle.trn.Z\n",
    "    * shuttle.tst\n",
    "2. These two files are the training and test set respectively. You'll have to uncompress the training file. On *nix based environments this would be \n",
    "    \n",
    "```$ uncompress shuttle.trn.Z ```\n",
    "\n",
    "Assumptions:\n",
    "1. We don't have a description of each of the features, so it's not possible to know what the correct range of values is for each feature.\n",
    "2. I'm assuming that all of the values for each feature are within a proper range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_names = [f\"f{i}\" for i in range(1, 10)]\n",
    "cols_names = np.append(feature_names, [\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_table('shuttle.trn', sep='\\s+', names=cols_names)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_table('shuttle.tst', sep='\\s+', names=cols_names)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some exploration of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inspect frequencies of different labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x=train_df['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'].value_counts(normalize=True).apply(lambda x: \"{:.3f}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['target'].value_counts(normalize=True).apply(lambda x: \"{:.3f}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset is very imbalanced. Frequencies for labels = [2, 3, 6, 7] are quite low, relative to labels=[1, 4, 5].\n",
    "But training and test sets have a similar frequency.*\n",
    "\n",
    "*Note: ~78% of labels are for label=1, so a model that always predicts label=1 for the target, would have an accuracy of 78%. We need to use precision/recall or another metric that handles imbalanced datasets well, when evaluating our model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at data in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    _ = train_df.hist(figsize=(20,15), grid=False, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.histplot(train_df['f2'], kde=True, stat=\"density\", linewidth=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['f2'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*That's interesting, the IQR (75pct - 25pct) is 0. Values are mostly 0, but some Max and min values on the extremes are pulling the mean left*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f2'], x=train_df[\"target\"].values).set_title(\"Boxplot of target against f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.histplot(train_df['f4'], kde=True, stat=\"density\", linewidth=0 ).set_title(\"Distribution of f4 in training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['f4']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Similar to f2, values for f4 are mostly 0, except for some extremes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df[train_df['f4'] == 0].shape)\n",
    "print(train_df[train_df['f4'] == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['target'].value_counts(normalize=True).apply(lambda x: \"{:.3f}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4_zero_mask = train_df['f4'] == 0\n",
    "\n",
    "f4_zeros = train_df[f4_zero_mask]['target'].value_counts(normalize=False)\n",
    "f4_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4_non_zeros = train_df[~f4_zero_mask]['target'].value_counts(normalize=False)\n",
    "f4_non_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at ratios between targets with 0s and non 0s...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4_non_zeros.divide(f4_zeros, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For f4, all the targets with label=2, have a zero. label=6 are equally likely to have a zero or non-zero value. labels=[1, 3, 4] are less likely to have a non-zero value. label=7 is much more likely to have a non-zero value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.histplot(train_df['f6'], kde=True, stat=\"density\", linewidth=0 ).set_title(\"Distribution of f6 in training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f6'], x=train_df[\"target\"].values).set_title(\"Boxplot of target against f6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 10))\n",
    "_ = sns.heatmap(train_df.corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*f2, f4, f6 (the features examined above) are showing little correlation with the target*\n",
    "\n",
    "*f1, f5, f7, f8, f9 show medium to high correlation with target*\n",
    "\n",
    "*f1 & f7 have strong negative correlation. f5 has strong negative correlation with f8 & f9. f8 and f9 are positively correlated with one another.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f1'], x=train_df[\"target\"].values).set_title(\"Boxplot of f1 against against target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f5'], x=train_df[\"target\"].values).set_title(\"Boxplot of f5 against target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Possible outliers in f5, for labels=[1,5]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f7'], x=train_df[\"target\"].values).set_title(\"Boxplot of f7 against target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f8'], x=train_df[\"target\"].values).set_title(\"Boxplot of f8 against target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Possible outliers in f8, for labels=[1,5]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(y=train_df['f9'], x=train_df[\"target\"].values).set_title(\"Boxplot of f9 against target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Possible outliers in f9, for labels=[1,5]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using zscores to measure possible outliers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "z = np.abs(stats.zscore(train_df[['f5', 'f8', 'f9']]))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_df[['f5', 'f8', 'f9','target']].copy()\n",
    "scores['zscore_f5'] = z[:, 0]\n",
    "scores['zscore_f8'] = z[:, 1]\n",
    "scores['zscore_f9'] = z[:, 2]\n",
    "threshold = 4\n",
    "mask_outlier_candidates = (scores['zscore_f5'] > threshold) | (scores['zscore_f8'] > threshold)\n",
    "scores[mask_outlier_candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[mask_outlier_candidates].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(x=scores[mask_outlier_candidates]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(train_df[mask_outlier_candidates].index.values, axis=0, inplace=True)\n",
    "train_df.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The outlier values for f4, f8 occur over targets 1, 4, 5 similarly. Without knowing the correct range of values foreach feature, it's difficult to know if the these values are true outliers or not, and whether it's appropriate to replace them, or drop them. There are 11 of these rows. For the moment I will drop them.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[(scores['zscore_f9'] > threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(x=scores[(scores['zscore_f9'] > 4) & (~mask_outlier_candidates)]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All the values f9 that have an extreme value, have a target of label=5. This is interesting, and a potentially useful piece of information for the model.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "_ = sns.scatterplot(x=\"f8\", y=\"f9\", \n",
    "                    hue='target', \n",
    "                    style='target',\n",
    "                    palette=\"Set2\",\n",
    "                    legend='full',                    \n",
    "                    data=train_df).set_title(\"Scatter f8 against f9 with target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "_ = sns.scatterplot(x=\"f1\", y=\"f7\", \n",
    "                    hue='target', \n",
    "                    style='target',\n",
    "                    palette=\"Set2\",\n",
    "                    legend='full',                    \n",
    "                    data=train_df).set_title(\"Scatter f7 against f1 with target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation on Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Going to try a Random forest Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df[feature_names]\n",
    "y = train_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "_ = clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set: {:.3f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Validation set: {:.3f}\".format(clf.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "_ = clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set: {:.3f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Validation set: {:.3f}\".format(clf.score(X_val, y_val)))\n",
    "print(classification_report(y_val, clf.predict(X_val), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Very good scores, except for class=6 which has poor recall.* \n",
    "*Going to add some under and over sampling with SMOTETomek*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTETomek(smote=SMOTE(k_neighbors=2, sampling_strategy={  \n",
    "                                            2: 50, \n",
    "                                            7: 50, \n",
    "                                            6: 50}))\n",
    "\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "_ = clf.fit(X_smote, y_smote)\n",
    "\n",
    "print(classification_report(y_val, clf.predict(X_val), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The results look much better now for labels 6 and 7. It could be that we've just being lucky with the training/validation splits, so going to run using cv splits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Let's run the training set through cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "#sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('sampling', SMOTETomek(smote=SMOTE(k_neighbors=3, sampling_strategy={  \n",
    "                                            2: 50, \n",
    "                                            7: 50, \n",
    "                                            6: 50}))),\n",
    "   ('model', RandomForestClassifier(n_estimators=10, random_state=42))\n",
    "])\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #Using recall_macro & precision_macro to give equal weight to each class.\n",
    "    scores = cross_validate(pipeline, X, y, scoring=['recall_macro', 'precision_macro'], cv=5) \n",
    "\n",
    "print(f'Mean recall_macro score: {np.mean(scores[\"test_recall_macro\"]):.3f}')\n",
    "print(f'Mean precision_macro score: {np.mean(scores[\"test_precision_macro\"]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These macro recall and precision scores are similar to what we got for the classification_report. \n",
    "\"Macro\" recall gives equal weighting to all labels, regardless of the number of samples of each label. So if you have an imbalanced dataset, macro recall will capture classifiction misses*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Score: Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = test_df['target'] \n",
    "y_hat = pipeline.fit(X,y).predict(test_df[feature_names]) #Fit all of X, y now that we have model built.\n",
    "print(classification_report(y_actual, y_hat, digits=3))\n",
    "clf.score(test_df[feature_names], y_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The RF model has performed well on the test set also, achieving an overall accuracy of 0.9997*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
